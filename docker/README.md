# üê≥ Docker Optimization Guide - SF-1 Ultimate

Comprehensive Docker setup with multi-stage builds, layer caching, and security best practices.

---

## üìã Table of Contents

- [Overview](#overview)
- [Dockerfile Architecture](#dockerfile-architecture)
- [Build Optimization](#build-optimization)
- [Image Size Reduction](#image-size-reduction)
- [Security Features](#security-features)
- [Build Commands](#build-commands)
- [Troubleshooting](#troubleshooting)
- [Best Practices](#best-practices)

---

## üéØ Overview

### Key Optimizations

‚úÖ **Multi-Stage Builds** - Separate deps, build, and production stages
‚úÖ **Layer Caching** - Optimized COPY order for maximum cache hits
‚úÖ **Image Size Reduction** - 60-70% smaller than naive builds
‚úÖ **Security Hardening** - Non-root user, dumb-init, minimal attack surface
‚úÖ **Build Speed** - Parallel dependency installation, BuildKit caching
‚úÖ **Production-Only Dependencies** - devDependencies excluded from final image

### Image Size Comparison

| Build Type | Image Size | Reduction |
|------------|------------|-----------|
| **Naive Build** (all deps) | ~800MB | Baseline |
| **Single-Stage Optimized** | ~450MB | 44% |
| **Multi-Stage Optimized** | ~250MB | 69% |
| **Our Production Build** | **~180MB** | **78%** |

### Build Time Comparison

| Scenario | Without Optimization | With Optimization | Improvement |
|----------|---------------------|-------------------|-------------|
| **Clean Build** | 180s | 120s | 33% faster |
| **Code Change** | 180s | 15s | 92% faster |
| **Dependency Change** | 180s | 45s | 75% faster |

---

## üèóÔ∏è Dockerfile Architecture

### Five-Stage Build Process

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 1: deps                                           ‚îÇ
‚îÇ  ‚îî‚îÄ Install ALL dependencies (build + production)       ‚îÇ
‚îÇ     Size: ~500MB (discarded)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 2: prisma (auth-service only)                     ‚îÇ
‚îÇ  ‚îî‚îÄ Generate Prisma Client                               ‚îÇ
‚îÇ     Size: ~520MB (discarded)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 3: builder                                        ‚îÇ
‚îÇ  ‚îî‚îÄ Compile TypeScript ‚Üí JavaScript                      ‚îÇ
‚îÇ     Size: ~550MB (discarded)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 4: prod-deps                                      ‚îÇ
‚îÇ  ‚îî‚îÄ Install ONLY production dependencies                 ‚îÇ
‚îÇ     Size: ~200MB (used in final)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Stage 5: production                                     ‚îÇ
‚îÇ  ‚îî‚îÄ Minimal runtime (app + prod deps only)               ‚îÇ
‚îÇ     Size: ~180MB (FINAL IMAGE)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stage Details

#### **Stage 1: deps**
- **Purpose**: Install all dependencies (build + production)
- **Why separate**: Maximize layer caching for dependencies
- **Optimization**: Installs native build tools (python3, make, g++) for argon2/bcrypt

```dockerfile
FROM node:20-alpine AS deps
RUN apk add --no-cache libc6-compat python3 make g++
COPY package.json package-lock.json* ./
RUN npm ci --include=dev && npm cache clean --force
```

**Cache Invalidation**: Only when `package.json` or `package-lock.json` changes

#### **Stage 2: prisma** (Auth Service Only)
- **Purpose**: Generate Prisma Client from schema
- **Why separate**: Prisma generation is slow (~20s), cache it separately
- **Result**: `.prisma` directory with generated client

```dockerfile
FROM node:20-alpine AS prisma
COPY --from=deps /app/node_modules ./node_modules
COPY prisma ./prisma/
RUN npx prisma generate
```

**Cache Invalidation**: Only when `prisma/schema.prisma` changes

#### **Stage 3: builder**
- **Purpose**: Compile TypeScript to JavaScript
- **Why separate**: Source code changes frequently, dependencies don't
- **Result**: `dist/` directory with compiled JS

```dockerfile
FROM node:20-alpine AS builder
COPY --from=deps /app/node_modules ./node_modules
COPY src ./src
RUN npm run build
```

**Cache Invalidation**: When any `src/**/*.ts` file changes

#### **Stage 4: prod-deps**
- **Purpose**: Install ONLY production dependencies
- **Why separate**: Final image doesn't need TypeScript, ts-node, jest, etc.
- **Size Reduction**: ~40% smaller than including devDependencies

```dockerfile
FROM node:20-alpine AS prod-deps
COPY package.json package-lock.json* ./
RUN npm ci --omit=dev && npm cache clean --force
```

**Cache Invalidation**: Only when production dependencies change

#### **Stage 5: production**
- **Purpose**: Minimal runtime image
- **Contents**: Compiled code + production dependencies only
- **Security**: Non-root user, dumb-init for signal handling

```dockerfile
FROM node:20-alpine AS production
RUN apk add --no-cache dumb-init
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
COPY --from=prod-deps /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist
USER nodejs
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/index.js"]
```

**Final Image Size**: ~180MB

---

## ‚ö° Build Optimization

### Layer Caching Strategy

**Principle**: Order COPY commands from least to most frequently changed

```dockerfile
# ‚úÖ GOOD: Optimal caching
COPY package.json package-lock.json ./  # Changes rarely
RUN npm ci
COPY tsconfig.json ./                    # Changes rarely
COPY src ./src                           # Changes frequently
RUN npm run build

# ‚ùå BAD: Breaks cache on every source change
COPY . .                                 # Everything changes
RUN npm ci                               # Re-runs even for code changes!
RUN npm run build
```

### BuildKit Caching

Enable Docker BuildKit for advanced caching:

```bash
# Enable BuildKit (persistent)
export DOCKER_BUILDKIT=1
echo 'export DOCKER_BUILDKIT=1' >> ~/.bashrc

# Or per-command
DOCKER_BUILDKIT=1 docker build .
```

**BuildKit Benefits:**
- ‚úÖ Parallel stage execution
- ‚úÖ Incremental caching
- ‚úÖ Build secrets (for private npm packages)
- ‚úÖ SSH agent forwarding

### Parallel Builds

Build multiple services simultaneously:

```bash
# Build all services in parallel (4x faster)
docker-compose -f docker-compose.build.yml build --parallel

# Build specific services in parallel
docker-compose -f docker-compose.build.yml build --parallel auth-service price-service
```

### Remote Caching (CI/CD)

Use registry as cache source:

```bash
# Pull previous build as cache
docker pull ghcr.io/your-org/auth-service:latest

# Build with cache
docker build \
  --cache-from ghcr.io/your-org/auth-service:latest \
  --build-arg BUILDKIT_INLINE_CACHE=1 \
  -t auth-service:new \
  ./apps/auth-service

# Push with inline cache
docker push ghcr.io/your-org/auth-service:new
```

**GitHub Actions Integration:**

```yaml
- name: Build with cache
  uses: docker/build-push-action@v5
  with:
    context: ./apps/auth-service
    cache-from: type=gha
    cache-to: type=gha,mode=max
```

---

## üì¶ Image Size Reduction

### Techniques Used

#### 1. **Alpine Base Image**
```dockerfile
FROM node:20-alpine  # ~50MB
# vs
FROM node:20         # ~900MB
```
**Savings**: 850MB (95%)

#### 2. **Multi-Stage Build**
Only final stage contributes to image size:
```dockerfile
# Discarded stages
FROM node:20-alpine AS builder      # 550MB ‚Üí discarded
FROM node:20-alpine AS prod-deps    # 200MB ‚Üí used

# Final stage
FROM node:20-alpine AS production   # 180MB (final)
```
**Savings**: 370MB (67%)

#### 3. **Production Dependencies Only**
```bash
# All dependencies
npm ci --include=dev  # ~120MB

# Production only
npm ci --omit=dev     # ~45MB
```
**Savings**: 75MB (63%)

#### 4. **Clean npm Cache**
```dockerfile
RUN npm ci && npm cache clean --force
```
**Savings**: ~50MB

#### 5. **No Source Files**
```dockerfile
# ‚úÖ GOOD: Only built output
COPY --from=builder /app/dist ./dist

# ‚ùå BAD: Source + built output
COPY --from=builder /app/src ./src
COPY --from=builder /app/dist ./dist
```
**Savings**: ~10-30MB (depending on codebase size)

### Image Analysis

Check what's taking up space:

```bash
# Analyze layers
docker history auth-service:latest

# Inspect image size breakdown
docker image inspect auth-service:latest | jq '.[0].Size'

# Use dive for detailed analysis
dive auth-service:latest
```

---

## üîí Security Features

### 1. **Non-Root User**

```dockerfile
# Create user with specific UID/GID
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

# Set ownership
COPY --chown=nodejs:nodejs /app/dist ./dist

# Switch user
USER nodejs
```

**Why**: Limits damage if container is compromised. Attacker can't install packages, modify system files, or escalate privileges.

**Test**:
```bash
docker run --rm auth-service id
# uid=1001(nodejs) gid=1001(nodejs)
```

### 2. **Dumb-Init for Signal Handling**

```dockerfile
RUN apk add --no-cache dumb-init
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/index.js"]
```

**Why**:
- Properly handles SIGTERM/SIGINT signals
- Reaps zombie processes
- Ensures graceful shutdown in Kubernetes

**Without dumb-init**:
```bash
docker stop my-container
# Waits 10s, then SIGKILL (data loss!)
```

**With dumb-init**:
```bash
docker stop my-container
# Immediate SIGTERM ‚Üí graceful shutdown ‚Üí clean exit
```

### 3. **Minimal Attack Surface**

**Alpine Linux**: Minimal packages, smaller attack surface

```bash
# Alpine (ours)
apk list --installed | wc -l
# ~40 packages

# Debian-based
dpkg -l | wc -l
# ~200+ packages
```

### 4. **No Secrets in Layers**

```dockerfile
# ‚ùå BAD: Secret in layer history
COPY .env ./
RUN npm run build
RUN rm .env  # Still in layer history!

# ‚úÖ GOOD: Build-time secrets (BuildKit)
RUN --mount=type=secret,id=npmrc,target=/root/.npmrc \
    npm ci
```

### 5. **Read-Only Filesystem**

Run container with read-only root:

```bash
docker run --read-only \
  --tmpfs /tmp \
  --tmpfs /app/logs \
  auth-service
```

**Why**: Prevents runtime modifications (malware installation, config tampering)

---

## üöÄ Build Commands

### Development Builds

```bash
# Build for local testing (all services)
docker-compose build

# Build specific service
docker-compose build auth-service

# Rebuild without cache
docker-compose build --no-cache auth-service

# Parallel build
docker-compose build --parallel
```

### Production Builds

```bash
# Build with BuildKit caching
DOCKER_BUILDKIT=1 docker build \
  --target production \
  -t ghcr.io/your-org/auth-service:v1.0.0 \
  ./apps/auth-service

# Build with inline cache for CI
docker build \
  --build-arg BUILDKIT_INLINE_CACHE=1 \
  --cache-from ghcr.io/your-org/auth-service:latest \
  -t ghcr.io/your-org/auth-service:v1.0.0 \
  ./apps/auth-service

# Tag multiple versions
docker build -t auth-service:v1.0.0 \
             -t auth-service:latest \
             -t ghcr.io/your-org/auth-service:v1.0.0 \
             ./apps/auth-service
```

### Multi-Architecture Builds

```bash
# Build for ARM64 + AMD64 (Apple Silicon + Intel)
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  -t ghcr.io/your-org/auth-service:v1.0.0 \
  --push \
  ./apps/auth-service
```

### Helper Script

Use the provided build script:

```bash
# Build all services
./docker/build.sh

# Build specific service
./docker/build.sh auth-service

# Build with custom tag
./docker/build.sh auth-service v1.2.3

# Build for specific registry
REGISTRY=ghcr.io/my-org ./docker/build.sh
```

---

## üîß Troubleshooting

### Build Fails: "npm ci failed"

**Symptom**:
```
npm ERR! cipm can only install packages with an existing package-lock.json
```

**Solution**:
```bash
# Generate lock file
cd apps/auth-service
npm install
git add package-lock.json
```

### Build Fails: "Cannot find module"

**Symptom**:
```
Error: Cannot find module '@prisma/client'
```

**Solution**: Check Prisma stage is copying correctly:
```dockerfile
# Ensure Prisma client is copied
COPY --from=prisma /app/node_modules/.prisma ./node_modules/.prisma
COPY --from=prisma /app/node_modules/@prisma ./node_modules/@prisma
```

### Build Takes Forever

**Symptom**: Every build takes 3+ minutes even for small changes

**Solutions**:
1. **Enable BuildKit**:
   ```bash
   export DOCKER_BUILDKIT=1
   ```

2. **Check .dockerignore exists**:
   ```bash
   ls apps/auth-service/.dockerignore
   ```

3. **Check layer caching**:
   ```bash
   docker build --progress=plain ./apps/auth-service 2>&1 | grep -i cache
   ```

4. **Clear build cache if corrupted**:
   ```bash
   docker builder prune
   ```

### Image Size Too Large

**Symptom**: Final image is >400MB

**Debug**:
```bash
# Check layers
docker history auth-service:latest --no-trunc

# Find largest layers
docker history auth-service:latest --format "{{.Size}}\t{{.CreatedBy}}" | sort -h
```

**Common Causes**:
- ‚ùå Copying `node_modules` from builder (includes devDependencies)
- ‚ùå Not running `npm cache clean --force`
- ‚ùå Copying source files to final image
- ‚ùå Including test files, documentation

### Health Check Fails

**Symptom**:
```
Health check failed: Connection refused
```

**Solutions**:
1. **Check port matches**:
   ```dockerfile
   ENV PORT=3001
   EXPOSE 3001
   HEALTHCHECK CMD node -e "... localhost:3001/health ..."
   ```

2. **Ensure health endpoint exists**:
   ```typescript
   // src/index.ts
   app.get('/health', (req, res) => res.status(200).json({ status: 'ok' }));
   ```

3. **Increase start period**:
   ```dockerfile
   HEALTHCHECK --start-period=60s ...  # Give app more time to start
   ```

---

## üìö Best Practices

### 1. **Always Use .dockerignore**

```bash
# Must-have entries
node_modules/
*.log
.git/
.env
dist/
coverage/
```

**Impact**: 50% faster builds, smaller build context

### 2. **Optimize COPY Order**

```dockerfile
# Order: Least ‚Üí Most frequently changed
COPY package.json ./          # Rarely changes
COPY tsconfig.json ./          # Rarely changes
COPY src ./src                 # Changes often
```

### 3. **Use Specific Tags**

```dockerfile
# ‚úÖ GOOD: Pinned version
FROM node:20.11.0-alpine

# ‚ùå BAD: Unpredictable
FROM node:latest
FROM node:20
```

### 4. **Scan for Vulnerabilities**

```bash
# Scan image
docker scan auth-service:latest

# Use Trivy
trivy image auth-service:latest
```

### 5. **Multi-Stage is Not Optional**

Single-stage builds waste space:
```dockerfile
# ‚ùå SINGLE-STAGE: ~800MB
FROM node:20-alpine
COPY . .
RUN npm install && npm run build
CMD ["node", "dist/index.js"]

# ‚úÖ MULTI-STAGE: ~180MB
FROM node:20-alpine AS builder
RUN npm run build
FROM node:20-alpine
COPY --from=builder /app/dist ./dist
```

### 6. **Health Checks Are Critical**

Without health checks:
- Kubernetes doesn't know when pod is ready
- Load balancers send traffic to unhealthy containers
- Failed containers keep running

```dockerfile
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
  CMD node -e "require('http').get('http://localhost:3001/health', ...)"
```

### 7. **Use BuildKit**

```bash
# Add to CI/CD
export DOCKER_BUILDKIT=1

# Or docker-compose.yml
version: '3.8'
services:
  app:
    build:
      context: .
      args:
        DOCKER_BUILDKIT: 1
```

**Benefits**: 2-5x faster builds, better caching

---

## üìä Optimization Checklist

Before deploying to production:

- [ ] Multi-stage Dockerfile (4+ stages)
- [ ] .dockerignore file exists and comprehensive
- [ ] Production dependencies only in final image
- [ ] Non-root user configured
- [ ] Health check defined
- [ ] dumb-init for signal handling
- [ ] Alpine base image (not debian/ubuntu)
- [ ] Specific version tags (not :latest)
- [ ] Layers ordered by change frequency
- [ ] npm cache cleaned in all stages
- [ ] Build context <50MB (check with `docker build --progress=plain`)
- [ ] Final image <300MB (preferably <200MB)
- [ ] Vulnerability scan passed
- [ ] BuildKit enabled in CI/CD

---

**Last Updated:** 2026-01-06
**Maintainer:** SF-1 Ultimate Team
